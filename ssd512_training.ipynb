{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dfd7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger\n",
    "from tensorflow.keras import backend as K\n",
    "from math import ceil\n",
    "from models.keras_ssd512 import ssd_512\n",
    "from keras_loss_function.keras_ssd_loss import SSDLoss\n",
    "from ssd_encoder_decoder.ssd_input_encoder import SSDInputEncoder\n",
    "from data_generator.object_detection_2d_data_generator import DataGenerator\n",
    "from data_generator.object_detection_2d_geometric_ops import Resize\n",
    "from data_generator.object_detection_2d_photometric_ops import ConvertTo3Channels\n",
    "from data_generator.data_augmentation_chain_original_ssd import SSDDataAugmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242351e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 512\n",
    "img_width = 512\n",
    "img_channels = 3\n",
    "mean_color = [123, 117, 104]\n",
    "swap_channels = [2, 1, 0]\n",
    "n_classes = 4\n",
    "scales_pascal = [0.3, 0.15, 0.07, 0.04]\n",
    "scales = scales_pascal\n",
    "aspect_ratios = [[1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0],\n",
    "                 [1.0, 2.0, 0.5, 3.0, 1.0/3.0]]\n",
    "two_boxes_for_ar1 = True\n",
    "steps = [16, 8, 4]\n",
    "offsets = [0.5, 0.5, 0.5]\n",
    "clip_boxes = False\n",
    "variances = [0.1, 0.1, 0.2, 0.2]\n",
    "normalize_coords = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26e63ded",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasTensor' object has no attribute '_keras_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m K\u001b[38;5;241m.\u001b[39mclear_session()\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mssd_512\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_channels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43ml2_regularization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mscales\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscales\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43maspect_ratios_per_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtwo_boxes_for_ar1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtwo_boxes_for_ar1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                \u001b[49m\u001b[43moffsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                \u001b[49m\u001b[43mclip_boxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_boxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvariances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariances\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[43mnormalize_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_coords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubtract_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                \u001b[49m\u001b[43mswap_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswap_channels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/SWIPENet-master/models/keras_ssd512.py:204\u001b[0m, in \u001b[0;36mssd_512\u001b[0;34m(image_size, n_classes, mode, l2_regularization, min_scale, max_scale, scales, aspect_ratios_global, aspect_ratios_per_layer, two_boxes_for_ar1, steps, offsets, clip_boxes, variances, coords, normalize_coords, subtract_mean, divide_by_stddev, swap_channels, confidence_thresh, iou_threshold, top_k, nms_max_output_size, return_predictor_sizes)\u001b[0m\n\u001b[1;32m    200\u001b[0m deconv3_mbox_loc \u001b[38;5;241m=\u001b[39m Conv2D(n_boxes[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m4\u001b[39m, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhe_normal\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_regularizer\u001b[38;5;241m=\u001b[39ml2(l2_reg), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeconv3_mbox_loc\u001b[39m\u001b[38;5;124m'\u001b[39m)(deconv3)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Generate the anchor boxes (called \"priors\" in the original Caffe/C++ implementation, so I'll keep their layer names)\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# Output shape of anchors: `(batch, height, width, n_boxes, 8)`\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m deconv1_mbox_priorbox \u001b[38;5;241m=\u001b[39m \u001b[43mAnchorBoxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscales\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscales\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect_ratios\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mtwo_boxes_for_ar1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtwo_boxes_for_ar1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffsets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_boxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_boxes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mvariances\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_coords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize_coords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdeconv1_mbox_priorbox\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeconv1_mbox_loc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m deconv2_mbox_priorbox \u001b[38;5;241m=\u001b[39m AnchorBoxes(img_height, img_width, this_scale\u001b[38;5;241m=\u001b[39mscales[\u001b[38;5;241m1\u001b[39m], next_scale\u001b[38;5;241m=\u001b[39mscales[\u001b[38;5;241m2\u001b[39m], aspect_ratios\u001b[38;5;241m=\u001b[39maspect_ratios[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    208\u001b[0m                                 two_boxes_for_ar1\u001b[38;5;241m=\u001b[39mtwo_boxes_for_ar1, this_steps\u001b[38;5;241m=\u001b[39msteps[\u001b[38;5;241m1\u001b[39m], this_offsets\u001b[38;5;241m=\u001b[39moffsets[\u001b[38;5;241m1\u001b[39m], clip_boxes\u001b[38;5;241m=\u001b[39mclip_boxes,\n\u001b[1;32m    209\u001b[0m                                 variances\u001b[38;5;241m=\u001b[39mvariances, coords\u001b[38;5;241m=\u001b[39mcoords, normalize_coords\u001b[38;5;241m=\u001b[39mnormalize_coords, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeconv2_mbox_priorbox\u001b[39m\u001b[38;5;124m'\u001b[39m)(deconv2_mbox_loc)\n\u001b[1;32m    210\u001b[0m deconv3_mbox_priorbox \u001b[38;5;241m=\u001b[39m AnchorBoxes(img_height, img_width, this_scale\u001b[38;5;241m=\u001b[39mscales[\u001b[38;5;241m2\u001b[39m], next_scale\u001b[38;5;241m=\u001b[39mscales[\u001b[38;5;241m3\u001b[39m], aspect_ratios\u001b[38;5;241m=\u001b[39maspect_ratios[\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m    211\u001b[0m                                     two_boxes_for_ar1\u001b[38;5;241m=\u001b[39mtwo_boxes_for_ar1, this_steps\u001b[38;5;241m=\u001b[39msteps[\u001b[38;5;241m2\u001b[39m], this_offsets\u001b[38;5;241m=\u001b[39moffsets[\u001b[38;5;241m2\u001b[39m], clip_boxes\u001b[38;5;241m=\u001b[39mclip_boxes,\n\u001b[1;32m    212\u001b[0m                                     variances\u001b[38;5;241m=\u001b[39mvariances, coords\u001b[38;5;241m=\u001b[39mcoords, normalize_coords\u001b[38;5;241m=\u001b[39mnormalize_coords, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeconv3_mbox_priorbox\u001b[39m\u001b[38;5;124m'\u001b[39m)(deconv3_mbox_loc)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/engine/base_layer.py:1037\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[1;32m   1040\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[0;32m~/Desktop/SWIPENet-master/keras_layers/keras_layer_AnchorBoxes.py:162\u001b[0m, in \u001b[0;36mAnchorBoxes.call\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    160\u001b[0m     batch_size, feature_map_height, feature_map_width, feature_map_channels \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39m_keras_shape\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# Not yet relevant since TensorFlow is the only supported backend right now, but it can't harm to have this in here for the future\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m     batch_size, feature_map_channels, feature_map_height, feature_map_width \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_keras_shape\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# Compute the grid of box center points. They are identical for all aspect ratios.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# Compute the step sizes, i.e. how far apart the anchor box center points will be vertically and horizontally.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthis_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KerasTensor' object has no attribute '_keras_shape'"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = ssd_512(image_size=(img_height, img_width, img_channels),\n",
    "                n_classes=n_classes,\n",
    "                mode='training',\n",
    "                l2_regularization=0.0005,\n",
    "                scales=scales,\n",
    "                aspect_ratios_per_layer=aspect_ratios,\n",
    "                two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                steps=steps,\n",
    "                offsets=offsets,\n",
    "                clip_boxes=clip_boxes,\n",
    "                variances=variances,\n",
    "                normalize_coords=normalize_coords,\n",
    "                subtract_mean=mean_color,\n",
    "                swap_channels=swap_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcb12617",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m weights_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/deeplearn/SWEIPENet/VGG_ILSVRC_16_layers_fc_reduced.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mload_weights(weights_path, by_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m adam \u001b[38;5;241m=\u001b[39m Adam(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, beta_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, beta_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.999\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-08\u001b[39m, decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[1;32m      4\u001b[0m ssd_loss \u001b[38;5;241m=\u001b[39m SSDLoss(neg_pos_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "weights_path = '/data/deeplearn/SWEIPENet/VGG_ILSVRC_16_layers_fc_reduced.h5'\n",
    "model.load_weights(weights_path, by_name=True)\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "ssd_loss = SSDLoss(neg_pos_ratio=3, alpha=1.0)\n",
    "model.compile(optimizer=adam, loss=ssd_loss.compute_loss)\n",
    "\n",
    "# Whether need sample weights\n",
    "VOC_2013_sampleweights_dir=None\n",
    "# VOC_2013_sampleweights_dir = '/data/deeplearn/VOCdevkit/VOC2013/Weights/'\n",
    "VOC_2013_images_dir = '/data/deeplearn/VOCdevkit/URPC2018/JPEGImages/'\n",
    "VOC_2013_annotations_dir = '/data/deeplearn/VOCdevkit/URPC2018/Annotations/'\n",
    "VOC_2013_trainval_image_set_filename = '/data/deeplearn/VOCdevkit/URPC2018/ImageSets/Main/trainval.txt'\n",
    "VOC_2013_test_image_set_filename = '/data/deeplearn/VOCdevkit/URPC2018/ImageSets/Main/test.txt'\n",
    "classes = ['background', 'seacucumber', 'seaurchin', 'scallop', 'starfish']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250eadda",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "val_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path=None)\n",
    "train_dataset.parse_xml(images_dirs=[VOC_2013_images_dir],\n",
    "                        image_set_filenames=[VOC_2013_trainval_image_set_filename],\n",
    "                        sample_weights_dirs=VOC_2013_sampleweights_dir,\n",
    "                        annotations_dirs=[VOC_2013_annotations_dir],\n",
    "                        classes=classes,\n",
    "                        include_classes='all',\n",
    "                        exclude_truncated=False,\n",
    "                        exclude_difficult=False,\n",
    "                        ret=False)\n",
    "val_dataset.parse_xml(images_dirs=[VOC_2013_images_dir],\n",
    "                      image_set_filenames=[VOC_2013_test_image_set_filename],\n",
    "                      annotations_dirs=[VOC_2013_annotations_dir],\n",
    "                      classes=classes,\n",
    "                      include_classes='all',\n",
    "                      exclude_truncated=False,\n",
    "                      exclude_difficult=True,\n",
    "                      ret=False)\n",
    "train_dataset.create_hdf5_dataset(file_path='dataset_pascal_voc_URPC2018_trainval.h5',\n",
    "                                  resize=False,\n",
    "                                  variable_image_size=True,\n",
    "                                  verbose=True)\n",
    "val_dataset.create_hdf5_dataset(file_path='dataset_pascal_voc_URPC2018_test.h5',\n",
    "                                resize=False,\n",
    "                                variable_image_size=True,\n",
    "                                verbose=True)\n",
    "\n",
    "# train_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path='/data/deeplearn/ssd_keras_sampleweight/dataset_pascal_voc_URPC2018_trainval.h5')\n",
    "# val_dataset = DataGenerator(load_images_into_memory=True, hdf5_dataset_path='/data/deeplearn/ssd_keras_sampleweight/dataset_pascal_voc_URPC2018_test.h5')\n",
    "\n",
    "batch_size = 16\n",
    "# For the training generator:\n",
    "ssd_data_augmentation = SSDDataAugmentation(img_height=img_height,\n",
    "                                            img_width=img_width,\n",
    "                                            background=mean_color)\n",
    "# For the validation generator:\n",
    "convert_to_3_channels = ConvertTo3Channels()\n",
    "resize = Resize(height=img_height, width=img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa67b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5: Instantiate an encoder that can encode ground truth labels into the format needed by the SSD loss function.\n",
    "predictor_sizes = [model.get_layer('deconv1_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('deconv2_mbox_conf').output_shape[1:3],\n",
    "                   model.get_layer('deconv3_mbox_conf').output_shape[1:3]]\n",
    "ssd_input_encoder = SSDInputEncoder(img_height=img_height,\n",
    "                                    img_width=img_width,\n",
    "                                    n_classes=n_classes,\n",
    "                                    predictor_sizes=predictor_sizes,\n",
    "                                    scales=scales,\n",
    "                                    aspect_ratios_per_layer=aspect_ratios,\n",
    "                                    two_boxes_for_ar1=two_boxes_for_ar1,\n",
    "                                    steps=steps,\n",
    "                                    offsets=offsets,\n",
    "                                    clip_boxes=clip_boxes,\n",
    "                                    variances=variances,\n",
    "                                    matching_type='multi',\n",
    "                                    pos_iou_threshold=0.5,\n",
    "                                    neg_iou_limit=0.5,\n",
    "                                    normalize_coords=normalize_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20334ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6: Create the generator handles that will be passed to Keras' `fit_generator()` function.\n",
    "train_generator = train_dataset.generate(batch_size=batch_size,\n",
    "                                         shuffle=True,\n",
    "                                         transformations=[ssd_data_augmentation],\n",
    "                                         label_encoder=ssd_input_encoder,\n",
    "                                         returns={'processed_images',\n",
    "                                                  'encoded_labels'},\n",
    "                                         keep_images_without_gt=False)\n",
    "val_generator = val_dataset.generate(batch_size=batch_size,\n",
    "                                     shuffle=False,\n",
    "                                     transformations=[convert_to_3_channels,\n",
    "                                                      resize],\n",
    "                                     label_encoder=ssd_input_encoder,\n",
    "                                     returns={'processed_images',\n",
    "                                              'encoded_labels'},\n",
    "                                     keep_images_without_gt=False)\n",
    "# Get the number of samples in the training and validations datasets.\n",
    "train_dataset_size = train_dataset.get_dataset_size()\n",
    "val_dataset_size = val_dataset.get_dataset_size()\n",
    "print(\"Number of images in the training dataset:\\t{:>6}\".format(train_dataset_size))\n",
    "print(\"Number of images in the validation dataset:\\t{:>6}\".format(val_dataset_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1172d01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a learning rate schedule.\n",
    "def lr_schedule(epoch):\n",
    "    if epoch < 80:\n",
    "        return 0.001\n",
    "    elif epoch<120:\n",
    "        return 0.0001\n",
    "    else:\n",
    "        return  0.0001\n",
    "\n",
    "# Define model callbacks.\n",
    "# TODO: Set the filepath under which you want to save the model.\n",
    "model_checkpoint = ModelCheckpoint(filepath='ssd512_URPC2018_epoch-{epoch:02d}.h5',\n",
    "                                   monitor='val_loss',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=False, # True\n",
    "                                   save_weights_only=False,\n",
    "                                   mode='auto',\n",
    "                                   period=1)\n",
    "csv_logger = CSVLogger(filename='ssd512_URPC2018_training_log_7_21.csv',\n",
    "                       separator=',',\n",
    "                       append=True)\n",
    "learning_rate_scheduler = LearningRateScheduler(schedule=lr_schedule,\n",
    "                                                verbose=1)\n",
    "terminate_on_nan = TerminateOnNaN()\n",
    "callbacks = [model_checkpoint,\n",
    "             csv_logger,\n",
    "             learning_rate_scheduler,\n",
    "             terminate_on_nan]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa33f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're resuming a previous training, set `initial_epoch` and `final_epoch` accordingly.\n",
    "initial_epoch = 0\n",
    "final_epoch = 220\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                              class_weight='auto',\n",
    "                              steps_per_epoch=steps_per_epoch,\n",
    "                              epochs=final_epoch,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=val_generator,\n",
    "                              validation_steps=ceil(val_dataset_size/batch_size),\n",
    "                              initial_epoch=initial_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623cf216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8aa29a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc68191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57f534a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
